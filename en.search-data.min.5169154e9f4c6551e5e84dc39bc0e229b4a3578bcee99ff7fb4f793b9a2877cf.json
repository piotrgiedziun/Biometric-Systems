[{"id":0,"href":"/biometric-systems/docs/course/","title":"Course","section":"Docs","content":"Course #  Contact #  Piotr Giedziun\n piotr.giedziun@pwr.edu.pl\nReporting requirements #  Technical report provide a record of your work and decision making. It should aggregate data and present the data. Use this guide to find out more.\n"},{"id":1,"href":"/biometric-systems/docs/labs/intro/","title":"Lab 1 - Intro","section":"Labs","content":"Introduction #  Introduction to Machine Learning and Python - required to complete the course.\n  Pre-read (Required) #    https://www.kaggle.com/learn/python (Lessons 1-6)  https://www.kaggle.com/learn/intro-to-deep-learning (Lessons 1-3)  https://www.kaggle.com/learn/computer-vision (Lessons 1-4)  Literature (Optional) #   Christopher M. Bishop. (2007). Pattern Recognition and Machine Learning Sebastian Raschka (2019). Python Machine Learning: Machine Learning and Deep Learning with Python John M. Zelle, Guido van Rossum. (2016). Python Programming: An Introduction to Computer Science  Demo project #  This is a handwritten digit recognizer created during the class. Draw a digit and click on \u0026ldquo;predict\u0026rdquo;.\n   let canvas, ctx; let mouseX, mouseY, mouseDown = 0; let touchX, touchY; let lastX, lastY = -1; function drawLine(ctx, x, y, size) { if (lastX == -1) { lastX = x; lastY = y; } ctx.strokeStyle = \"rgba(255, 255, 255, 1)\"; ctx.lineCap = \"round\"; ctx.beginPath(); ctx.moveTo(lastX, lastY); ctx.lineTo(x, y); ctx.lineWidth = size; ctx.stroke(); ctx.closePath(); lastX = x; lastY = y; } function clearCanvas(canvas, ctx) { ctx.clearRect(0, 0, canvas.width, canvas.height); document.getElementById(\"results\").innerHTML = \"\"; } function sketchpad_mouseDown() { mouseDown = 1; drawLine(ctx, mouseX, mouseY, 12); } function sketchpad_mouseUp() { mouseDown = 0; lastX = -1; lastY = -1; } function sketchpad_mouseMove(e) { getMousePos(e); if (mouseDown == 1) { drawLine(ctx, mouseX, mouseY, 12); } } function getMousePos(e) { if (e.offsetX) { mouseX = e.offsetX; mouseY = e.offsetY; } else if (e.layerX) { mouseX = e.layerX; mouseY = e.layerY; } } function sketchpad_touchStart() { getTouchPos(); drawLine(ctx, touchX, touchY, 12); e.preventDefault(); } function sketchpad_touchEnd() { lastX = -1; lastY = -1; } function sketchpad_touchMove(e) { getTouchPos(e); drawLine(ctx, touchX, touchY, 12); e.preventDefault(); } function getTouchPos(e) { if (e.touches) { if (e.touches.length == 1) { let touch = e.touches[0]; touchX = touch.pageX - touch.target.offsetLeft; touchY = touch.pageY - touch.target.offsetTop; } } } async function init() { canvas = document.getElementById(\"sketchpad\"); if (canvas.getContext) ctx = canvas.getContext(\"2d\"); if (ctx) { canvas.addEventListener(\"mousedown\", sketchpad_mouseDown, false); canvas.addEventListener(\"mousemove\", sketchpad_mouseMove, false); window.addEventListener(\"mouseup\", sketchpad_mouseUp, false); canvas.addEventListener(\"touchstart\", sketchpad_touchStart, false); canvas.addEventListener(\"touchend\", sketchpad_touchEnd, false); canvas.addEventListener(\"touchmove\", sketchpad_touchMove, false); } document.getElementById(\"load\").style.display = \"none\"; await tf.setBackend(\"wasm\"); model = await tf.loadLayersModel(\"../intro/model.json\"); document.getElementById(\"demo\").style.display = \"block\"; } function predict() { const imageData = ctx.getImageData(0, 0, 140, 140); let tfImg = tf.browser.fromPixels(imageData, 1); let smalImg = tf.image.resizeBilinear(tfImg, [28, 28]); smalImg = tf.cast(smalImg, \"float32\"); const tensor = smalImg .expandDims(1) .reshape([1, 28, 28, 1]) .div(tf.scalar(255)); const prediction = model.predict(tensor); const predictedValues = prediction.dataSync(); const res = document.createTextNode(`id\\tscore\\n`); results.appendChild(res); for (i = 0; i  .leftside { width: 150px; height: 200px; } #sketchpad { float: left; height: 140px; width: 140px; border: 2px solid #888; border-radius: 4px; position: relative; background-color: black; } #demo { display: none; } input[type=\"submit\"] { background-color: rgb(59, 59, 59); }        Class content #   Introduction to the Google Colab environment, characteristics of working in an interactive environment Introduction to Python. The basics required to complete the course Introduction to Machine Learning (mean square error and gradient descent) Construction of a simple neural network using the TensorFlow library Introduction of the concepts of accuracy, confusion matrix and optimizer Implementation of a neural network (multilayer perceptron) on the MNIST dataset (preparing a dataset, dividing data into a test and training set, intro to loss functions, graph analysis) Implementation of the convolutional network for the same dataset. Interactive overview of convolutional layers, max pooling, activation and fully connected (dense)  "},{"id":2,"href":"/biometric-systems/docs/labs/face/","title":"Lab 2 - Face","section":"Labs","content":"Face biometrics #  The class covers face detection, face recognition and methods of spoofing them with deep fakes.\n  Pre-read (Required) #    https://www.tensorflow.org/guide/keras/sequential_model  https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html  https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html  Demo project #   Class content #   Introduction to face detection algorithms (Histogram of Oriented Gradients, Haar Cascades, Deep Neural Network). The aim of the task is to get to know the methods and their strengths and weaknesses. Implementation of the HOG (Histogram of Oriented Gradients) algorithm to detect characteristic points of the face. Development and implementation of selected features and algorithms to verify the similarity (e.g. calculating the distance between face elements and their comparison). Implementation of the identification function based on the face pattern. A test set is provided for the task, on which the effectiveness of the solution should be verified. The function and algorithms used in this activity will be evaluated in the next class. The group with the highest score will receive additional points. Preparation of a set of training data for eye color recognition and model implementation. Comparison of the deep learning method with the traditional, previously implemented methods. Analysis of the safety and effectiveness of the implemented methods. Deep fake implementation and its practical applications.  "},{"id":3,"href":"/biometric-systems/docs/labs/iris/","title":"Lab 3 - Iris","section":"Labs","content":"Iris recognition #  Identifying the characteristics of the iris is a similar process to identifying it with the facial pattern and fingerprint. The system segments the image of the iris and then converts it into a pattern which is compared with the pattern. Iris readers often use an additional system to illuminate the eye with near-infrared light.\n  Pre-read (Required) #    https://www.tensorflow.org/addons/tutorials/losses_triplet  https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin  https://www.v7labs.com/blog/image-segmentation-guide  Demo project #    .leftside { width: 150px; height: 200px; } #sketchpad { float: left; height: 140px; width: 140px; border: 2px solid #888; border-radius: 4px; position: relative; background-color: black; } #demo { display: none; } input[type=\"submit\"] { background-color: rgb(59, 59, 59); }    let model, results, canvas, ctx, touchX, touchY; let mouseX, mouseY, mouseDown = 0; let lastX, lastY = -1; let maxX = 0; let maxY = 0; let minX = 9999; let minY = 9999; let images = []; function drawLine(ctx, x, y, size) { if (lastX === -1) { lastX = x; lastY = y; } ctx.strokeStyle = \"rgba(255, 255, 255, 1)\"; ctx.lineCap = \"round\"; ctx.beginPath(); ctx.moveTo(lastX, lastY); ctx.lineTo(x, y); ctx.lineWidth = size; ctx.stroke(); ctx.closePath(); lastX = x; lastY = y; if (x  maxX || !maxX) maxX = x; if (x maxY || !maxY) maxY = y; if (y { await tf.setBackend(backend); }); gui.add(state, \"maxFaces\", 1, 20, 1).onChange(async (val) = { model = null; }); gui.add(state, \"bbox\"); gui.add(state, \"mesh\"); } function dot(a, b) { var hasOwnProperty = Object.prototype.hasOwnProperty; var sum = 0; for (var key in a) { if (hasOwnProperty.call(a, key) \u0026\u0026 hasOwnProperty.call(b, key)) { sum += a[key] * b[key]; } } return sum; } function similarity(a, b) { var magnitudeA = Math.sqrt(dot(a, a)); var magnitudeB = Math.sqrt(dot(b, b)); return dot(a, b) / (magnitudeA * magnitudeB); } function loadImage(src) { return new Promise((resolve, reject) = { let img = new Image(); img.crossOrigin = \"anonymous\"; img.onload = () = resolve(img); img.onerror = reject; img.src = src; }); } async function predictAll() { const padding = 20; const imageData = ctx.getImageData( Math.max(0, minX - padding), Math.max(0, minY - padding), Math.min(140, maxX + padding), Math.min(140, maxY + padding) ); const a = await infer(imageData); const res = document.createTextNode(`id\\tdistance\\n`); results.appendChild(res); for (let i = 0; i { const IMAGE_SIZE = 28; img = tf.browser.fromPixels(img, 1); img = tf.image.resizeBilinear(img, [28, 28]); const normalized = tf.add( tf.div(tf.cast(img, \"float32\"), 255.0), 0.0 ); const batched = tf.reshape(normalized, [ -1, IMAGE_SIZE, IMAGE_SIZE, 1, ]); return model.predict(batched); }) .data(); } async function init() { canvas = document.getElementById(\"sketchpad\"); ctx = canvas.getContext(\"2d\"); results = document.getElementById(\"results\"); if (ctx) { canvas.addEventListener(\"mousedown\", sketchpad_mouseDown, false); canvas.addEventListener(\"mousemove\", sketchpad_mouseMove, false); window.addEventListener(\"mouseup\", sketchpad_mouseUp, false); canvas.addEventListener(\"touchstart\", sketchpad_touchStart, false); canvas.addEventListener(\"touchend\", sketchpad_touchEnd, false); canvas.addEventListener(\"touchmove\", sketchpad_touchMove, false); } document.getElementById(\"load\").style.display = \"none\"; await tf.setBackend(\"wasm\"); model = await tf.loadGraphModel(\"../iris/model.json\"); images = await Promise.all([ loadImage(\"../iris/mnist/0.jpg\"), loadImage(\"../iris/mnist/1.jpg\"), loadImage(\"../iris/mnist/2.jpg\"), loadImage(\"../iris/mnist/3.jpg\"), loadImage(\"../iris/mnist/4.jpg\"), loadImage(\"../iris/mnist/5.jpg\"), loadImage(\"../iris/mnist/6.jpg\"), loadImage(\"../iris/mnist/7.jpg\"), loadImage(\"../iris/mnist/8.jpg\"), loadImage(\"../iris/mnist/9.jpg\"), ]); document.getElementById(\"demo\").style.display = \"block\"; document.getElementById(\"clear\").addEventListener(\"click\", () = { clearCanvas(canvas, ctx); results.innerHTML = \"\"; }); document.getElementById(\"predict\").addEventListener(\"click\", () = { predictAll(); }); }         Class content #   Introduction to the segmentation and related metrics (IoU, Dice score). Analysis of classic algorithms for automatic detection and segmentation of the iris (Geodesic Active Contours, SuperPixel Segmentation - SPS, Hough Transform). Construction of the U-NET network for iris segmentation on the CASIA V 4.0 set. Transformation of the iris image to the Cartesian coordinate system Extracting features with the Gabor filter. Analysis of the influence of light on the pattern. Use Hamming distance to calculate the distance between vectors Build iris embedding (a vector that represents the features extracted from the iris). Create Deep Neural Network that creates a mapping from iris images to a compact Euclidean space where distances directly represent iris similarity.  "},{"id":4,"href":"/biometric-systems/docs/labs/voice/","title":"Lab 4 - Voice","section":"Labs","content":"Voice biometrics #  Identification and voice recognition are issues that have a number of practical applications in automation, authentication and security. It is a popular method of remote authorization thanks to its non-invasive and accessibility (e.g. telephone, personal computer). This method combines physical and behavioral elements. It uses a person\u0026rsquo;s acoustic features, which are shaped by biological features (e.g. the shape of the larynx) and by additional behavioral features such as stress, rhythm, intonation and vocabulary selection.\n  Pre-read (Required) #    https://azure.microsoft.com/en-us/services/cognitive-services/speaker-recognition/#features  https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speaker-recognition-overview#speaker-verification  https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html  Class content #   Implementation of audio sampling and visualization (e.g. MFFC, spectrogram). Introduction to 1D convolution and Recurrent Neural Networks. Implementation of Microsoft Azure - Speaker Recognition. Solution security analysis, attack vector through voice imitation, voice generation (deepfake).  "},{"id":5,"href":"/biometric-systems/docs/labs/keystroke/","title":"Lab 5 - Keystroke","section":"Labs","content":"Keystroke dynamics #  Identifying or confirming the identity of an individual based on the manner and the rhythm of typing on a keyboard. During the labolatory you will learn different approaches for Keystroke Dynamics (such as anomaly detection, clusterization, etc.).\nPre-read (Required) #   Wojciech Wodo, Marek Klonowski, Piotr Syga, \u0026ldquo;SOME REMARKS ON KEYSTROKE DYNAMICS: Global Survillience, Retriving Information and Simple Countermeasures\u0026rdquo;, Proceedings of 7th International Conference on Security and Cryptography SECRYPT 2012, Rome 2012, ISBN: 978-989-8565-24-2 Wojciech Wodo, Lucjan Hanzlik, \u0026ldquo;Identity Security in Biometric Systems Based on Keystroking\u0026rdquo;, Proceedings of the 8th International Conference on Security and Cryptography: SECRYPT 2013, Reykjavik 2013, ISBN: 978-989-8565-73-0 Guansong Pang, Chunhua Shen, Longbing Cao, Anton van den Hengel, Deep Learning for Anomaly Detection: A Review  "},{"id":6,"href":"/biometric-systems/docs/resources/","title":"Resources","section":"Docs","content":"Resources #    Course website - Wojciech Wodo, PhD  Getting started with Python  Intro to Machine Learning  "}]